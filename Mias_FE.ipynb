{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mias FE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KheZWpg0RZEk"
      },
      "source": [
        "# importing the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# for reading and displaying images\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "# for creating validation set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# for evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "# torchvision for pre-trained models\n",
        "from torchvision import models\n",
        "import argparse\n",
        "from collections import Counter\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksqExTN75tj2",
        "outputId": "0a10d6b5-1413-4a20-85c8-79604574d98a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46IQFXg1M2X5"
      },
      "source": [
        "import os\n",
        "\n",
        "import cv2\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk6iW4CX23ta",
        "outputId": "330066e3-fb39-484b-a0f4-f855df4b67ed"
      },
      "source": [
        "print(os.listdir('/content/gdrive/MyDrive/mini-MIAS/images_processed'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fatty', 'fatty_glandular', 'dense_glandular']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA_XRBC3T6xf",
        "outputId": "4b80f854-d8df-43a1-e2c1-972318991f9b"
      },
      "source": [
        "# Make the validation directories\r\n",
        "try:\r\n",
        "    os.makedirs(\"/content/gdrive/MyDrive/mini-MIAS/val/fatty\")\r\n",
        "    os.makedirs(\"/content/gdrive/MyDrive/mini-MIAS/val/fatty_glandular\")\r\n",
        "    os.makedirs(\"/content/gdrive/MyDrive/mini-MIAS/val/dense_glandular\")\r\n",
        "except OSError:\r\n",
        "    print (\"Creation of the directory %s failed\")\r\n",
        "else:\r\n",
        "    print (\"Successfully created the directory %s \")\r\n",
        "\r\n",
        "# Create validation folder\r\n",
        "fatty_train='/content/gdrive/MyDrive/mini-MIAS/images_processed/dense_glandular'\r\n",
        "cat_train = base_dir + \"train/Cat/\"\r\n",
        "cat_val = base_dir + \"val/Cat/\"\r\n",
        "dog_train = base_dir + \"train/Dog/\"\r\n",
        "dog_val = base_dir + \"val/Dog/\"\r\n",
        "\r\n",
        "# cat_files = os.listdir(cat_train)\r\n",
        "# dog_files = os.listdir(dog_train)\r\n",
        "\r\n",
        "# # This will put 1000 images from the two training folders\r\n",
        "# # into their respective validation folders\r\n",
        "\r\n",
        "# for f in cat_files:\r\n",
        "#     validationCatsSearchObj = re.search(\"5\\d\\d\\d\", f)\r\n",
        "#     if validationCatsSearchObj:\r\n",
        "#         shutil.move(f'{cat_train}/{f}', cat_val)\r\n",
        "\r\n",
        "# for f in dog_files:\r\n",
        "#     validationCatsSearchObj = re.search(\"5\\d\\d\\d\", f)\r\n",
        "#     if validationCatsSearchObj:\r\n",
        "#         shutil.move(f'{dog_train}/{f}', dog_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully created the directory %s \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69KRZRKC3LBx"
      },
      "source": [
        "from torchvision import transforms, datasets, models\n",
        "\n",
        "image_transforms = {\n",
        "    # Train uses data augmentation\n",
        "    'train':\n",
        "    transforms.Compose([\n",
        "        #transforms.RandomResizedCrop(size=256, scale=(0.95, 1.0)),\n",
        "        #transforms.RandomRotation(degrees=15),\n",
        "        # transforms.ColorJitter(),\n",
        "        #transforms.RandomHorizontalFlip(),\n",
        "        #transforms.CenterCrop(size=224),  # Image net standards\n",
        "        transforms.Resize((299,299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "        \n",
        "      \n",
        "    ]),\n",
        "\n",
        "    'val': transforms.Compose([\n",
        "       transforms.Resize((299,299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "\n",
        "    ]),\n",
        "\n",
        "    'test': transforms.Compose([\n",
        "       transforms.Resize((299,299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "\n",
        "    ]),\n",
        "  \n",
        "\n",
        "    }"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF79FX3k7wtw",
        "outputId": "504a1fed-6aef-4f91-c8cc-fa2b63f26118"
      },
      "source": [
        "# from torch.utils.data import DataLoader, sampler, random_split\n",
        "\n",
        "# batch_size = 16\n",
        "\n",
        "# all_data = datasets.ImageFolder('/content/gdrive/MyDrive/mini-MIAS/images_processed',transform=image_transforms['train'])\n",
        "# # print(len(all_data))\n",
        "# # train_data_len = int(len(all_data)*0.8)\n",
        "# # valid_data_len = int((len(all_data) - train_data_len)/2)\n",
        "# # test_data_len = int(len(all_data) - train_data_len - valid_data_len)\n",
        "# # train_data, val_data, test_data = random_split(all_data, [train_data_len, valid_data_len, test_data_len])\n",
        "# # train_data.dataset.transform = image_transforms['train']\n",
        "# # val_data.dataset.transform = image_transforms['val']\n",
        "# # test_data.dataset.transform = image_transforms['test']\n",
        "# # print(len(train_data), len(val_data), len(test_data))\n",
        "\n",
        "# train_loader = DataLoader(all_data, batch_size=batch_size, shuffle=True)\n",
        "# # val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "# # test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXtXmkOctPND",
        "outputId": "beb8bb34-21ba-4ef3-d1d3-48baba59e757"
      },
      "source": [
        "from torch.utils.data import DataLoader, sampler, random_split\r\n",
        "\r\n",
        "batch_size = 8\r\n",
        "\r\n",
        "all_data = datasets.ImageFolder('/content/gdrive/MyDrive/mini-MIAS/images_processed')\r\n",
        "print(len(all_data))\r\n",
        "train_data_len = int(len(all_data)*0.8)\r\n",
        "valid_data_len = int((len(all_data) - train_data_len)/2)\r\n",
        "test_data_len = int(len(all_data) - train_data_len - valid_data_len)\r\n",
        "train_data, val_data, test_data = random_split(all_data, [train_data_len, valid_data_len, test_data_len])\r\n",
        "train_data.dataset.transform = image_transforms['train']\r\n",
        "val_data.dataset.transform = image_transforms['train']\r\n",
        "test_data.dataset.transform = image_transforms['train']\r\n",
        "print(len(train_data), len(val_data), len(test_data))\r\n",
        "\r\n",
        "train_loader = DataLoader(train_data, batch_size=2, shuffle=True)\r\n",
        "\r\n",
        "val_loader = DataLoader(val_data, batch_size=8, shuffle=True)\r\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=True)\r\n",
        "print(len(train_loader),len(test_loader),len(val_loader))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "322\n",
            "257 32 33\n",
            "129 3 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUTDqIYr8UU5",
        "outputId": "79966f62-f8c9-494a-bf8d-bb5ef3742f2c"
      },
      "source": [
        "trainiter = iter(test_loader)\n",
        "features_train, labels_train = next(trainiter)\n",
        "print(features_train.shape, labels_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 3, 299, 299]) torch.Size([8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5L_5VglZFOv"
      },
      "source": [
        "model = models.resnet50(pretrained=True)\r\n",
        "# for param in model.parameters():\r\n",
        "#     param.requires_grad = False\r\n",
        "num_ftrs =model.fc.in_features\r\n",
        "model.fc = torch.nn.Linear(num_ftrs, 3)\r\n",
        "\r\n",
        "# num_ftrs\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXWR2U3M6BAD"
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=model.to(device)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_OYXqncw7SG"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWuJOWYJ9F13",
        "outputId": "ee0c9f3c-fdc9-4f2c-e14c-c83b5a72ac50"
      },
      "source": [
        "for name, child in model.named_children():\r\n",
        "    print(name)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1\n",
            "bn1\n",
            "relu\n",
            "maxpool\n",
            "layer1\n",
            "layer2\n",
            "layer3\n",
            "layer4\n",
            "avgpool\n",
            "fc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmLOIINhw7SK"
      },
      "source": [
        "# model = model.to(device)\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.optim import lr_scheduler\r\n",
        "\r\n",
        "\r\n",
        "from torch.optim import Adam\r\n",
        "\r\n",
        "optimizer=optim.SGD(model.parameters(),lr=0.001,weight_decay=0.0001)\r\n",
        "loss_func=nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "# loss_fn = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "# Observe that all parameters are being optimized\r\n",
        "# optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\r\n",
        "\r\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\r\n",
        "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTOSvG6Hb0ge"
      },
      "source": [
        "train_count=322\r\n",
        "test_count=33"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP8afoJJw7SL",
        "outputId": "c2442b10-ceaf-4e94-f0a8-00a80da23b3a"
      },
      "source": [
        "num_epochs=20\r\n",
        "\r\n",
        "best_accuracy=0.0\r\n",
        "pred=[]\r\n",
        "true=[]\r\n",
        "for epoch in range(num_epochs):\r\n",
        "\r\n",
        "  model.train()\r\n",
        "  train_accuracy=0.0\r\n",
        "  train_loss=0.0\r\n",
        "  \r\n",
        "\r\n",
        "  for i, (images,labels) in enumerate(train_loader):\r\n",
        "\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "    output=model(images)\r\n",
        "    loss=loss_func(output,labels)\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    train_loss+=loss.item()\r\n",
        "    \r\n",
        "    _, prediction=torch.max(output.data,1)\r\n",
        "\r\n",
        "        \r\n",
        "   \r\n",
        "    train_accuracy+=(prediction==labels).sum().item()\r\n",
        "\r\n",
        "    prediction=prediction.cpu().numpy()\r\n",
        "    labels=labels .cpu().numpy()\r\n",
        "    prediction = np.reshape(prediction,(len(prediction),1))\r\n",
        "    labels = np.reshape(labels,(len(prediction),1))\r\n",
        "\r\n",
        "    for i in range(len(prediction)):\r\n",
        "      pred.append(prediction[i])\r\n",
        "      true.append(labels[i])\r\n",
        "            \r\n",
        "        \r\n",
        "  train_accuracy=train_accuracy/train_count\r\n",
        "  train_loss=train_loss/train_count\r\n",
        "\r\n",
        "  model.eval()\r\n",
        "    \r\n",
        "  test_accuracy=0.0\r\n",
        "  for i, (images,labels) in enumerate(test_loader):\r\n",
        "      if torch.cuda.is_available():\r\n",
        "          images=Variable(images.cuda())\r\n",
        "          labels=Variable(labels.cuda())\r\n",
        "            \r\n",
        "      outputs=model(images)\r\n",
        "      _, prediction=torch.max(outputs.data,1)\r\n",
        "      test_accuracy+=(prediction==labels).sum().item()\r\n",
        " \r\n",
        "    \r\n",
        "  test_accuracy=test_accuracy/test_count\r\n",
        "    \r\n",
        "    \r\n",
        "  print('Epoch: ',epoch,' Train Loss: ',train_loss,' Train Accuracy: ',train_accuracy,'Test Accuracy: ',test_accuracy)\r\n",
        "\r\n",
        "  if test_accuracy>best_accuracy: \r\n",
        "        torch.save(model.state_dict(),'/content/gdrive/MyDrive/mini-MIAS/best_checkpoint50.model')\r\n",
        "        best_accuracy=test_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0  Train Loss:  0.4513636731212924  Train Accuracy:  0.2857142857142857 Test Accuracy:  0.36363636363636365\n",
            "Epoch:  1  Train Loss:  0.40806738948970106  Train Accuracy:  0.38509316770186336 Test Accuracy:  0.30303030303030304\n",
            "Epoch:  2  Train Loss:  0.3789783134038404  Train Accuracy:  0.45962732919254656 Test Accuracy:  0.30303030303030304\n",
            "Epoch:  3  Train Loss:  0.34457781475893456  Train Accuracy:  0.468944099378882 Test Accuracy:  0.6666666666666666\n",
            "Epoch:  4  Train Loss:  0.32566952376817326  Train Accuracy:  0.4906832298136646 Test Accuracy:  0.5757575757575758\n",
            "Epoch:  5  Train Loss:  0.34093833747117414  Train Accuracy:  0.4782608695652174 Test Accuracy:  0.7878787878787878\n",
            "Epoch:  6  Train Loss:  0.2983516359366245  Train Accuracy:  0.5279503105590062 Test Accuracy:  0.5454545454545454\n",
            "Epoch:  7  Train Loss:  0.30357375713238804  Train Accuracy:  0.5124223602484472 Test Accuracy:  0.7575757575757576\n",
            "Epoch:  8  Train Loss:  0.29094687034809813  Train Accuracy:  0.5527950310559007 Test Accuracy:  0.7575757575757576\n",
            "Epoch:  9  Train Loss:  0.29182116030165867  Train Accuracy:  0.5434782608695652 Test Accuracy:  0.5151515151515151\n",
            "Epoch:  10  Train Loss:  0.3034130119435165  Train Accuracy:  0.5403726708074534 Test Accuracy:  0.696969696969697\n",
            "Epoch:  11  Train Loss:  0.2729142386211742  Train Accuracy:  0.577639751552795 Test Accuracy:  0.696969696969697\n",
            "Epoch:  12  Train Loss:  0.2254307801771608  Train Accuracy:  0.6304347826086957 Test Accuracy:  0.696969696969697\n",
            "Epoch:  13  Train Loss:  0.26563255345414144  Train Accuracy:  0.5900621118012422 Test Accuracy:  0.6666666666666666\n",
            "Epoch:  14  Train Loss:  0.19020292759867188  Train Accuracy:  0.6490683229813664 Test Accuracy:  0.5454545454545454\n",
            "Epoch:  15  Train Loss:  0.2339194234900223  Train Accuracy:  0.6024844720496895 Test Accuracy:  0.6060606060606061\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}